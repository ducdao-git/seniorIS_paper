\section{nuImages Dataset}  \label{sec:nuimage}

The nuImages dataset is a large dataset that is designed for computer vision tasks in autonomous driving applications. It was developed by the Motional team and is available for personal and educational use. The dataset comprises 93,000 images that were carefully selected from 500 driving logs. Each log provides data from the entire sensor suite of an autonomous vehicle, including six cameras, one LIDAR, five RADARs, GPS, and IMU. The images in the nuImages dataset were selected based on two criteria. The first 75\% of the dataset consists of images that are challenging for object detection models to detect objects within them due to weather and lighting conditions, as well as rare classes like motorcycles and bicycles. These weather and lighting conditions are essential for autonomous driving applications as the vehicle needs to operate in rain, snow, storms, and during nighttime. The remaining 25\% of the dataset was sampled uniformly to avoid bias towards a particular class or condition. Using these two schemes ensures that the nuImages dataset has a balanced distribution of classes, weather, and lighting conditions.

The 93,000 images in the data set are divided into three distinct subsets. Precisely, the training subset consists of 67,000 images, the validation subset has 16,000 images, and the test subset comprises 10,000 images. Notably, the training and validation subsets contain annotated images, while the test subset does not. The dataset can be obtained at \cite{nuimages_dataset}. To use the nuImages dataset for evaluating the object detection model, we only concern ourselves with the Metadata and Sample directories in the dataset. The Metadata directory provides information about all the images in each subset, including the mapping between the image to their class, mask, and bounding box location, if available. Meanwhile, the Samples directory contains traffic scene images extracted from each camera mounted on the vehicle. 

In conjunction with the dataset, the Motional team also developed the nuscenes-devkit library to provide an API to simplify accessing these data. We build the \inlcode{NuImgSample} class in \inlcode{src/nuim\_util.py} to represent each image in the dataset. Each \inlcode{NuImgSample} object utilizes the NuImages object from the API to extract important information such as the original image path, mask, bounding box location, and classification label for each object in the image. During initialization, \inlcode{NuImgSample} also maps the nuImages object's label to one of the supported labels or removes the object entirely if it is not supported. The supported labels are bicycle, bus, car, motorcycle, person, and truck. The mapping from nuImages catogorical labels to supported label is available in \inlcode{src/label\_mapping.py}. This mapping and filtering process is crucial because the nuImages labels (ground-truth label set), Mask R-CNN labels (predicted label set 1), and YOLOv5 (predicted label set 2) are all different. This process ensures that the ground-truth and predicted labels match if the model correctly detects the object.

After creating instances of the NuImageSample class with the extracted images from the dataset, the \inlcode{get\_truth\_objs} function in \inlcode{src/nuim\_util.py} is used to process these instances and generate a list of TruthObject instances. Each TruthObject instance represents an object in the original image and contains essential information about the object such as its classification label, bounding box coordinates in the format ($x_{min}$, $y_{min}$, $x_{max}$, $y_{max}$), and the object's mask matrix. By using the TruthObject class to represent each object in the ground truth image, our code takes advantage of the benefits of object-oriented programming (OOP) design properties. This approach makes it easy to access the necessary information about each object and ensures that the code remains loosely coupled.