\section{Metric Implementation Detail}  \label{sec:metric_implement}

Recall from Section \ref{sec:eval_metrics}, to evaluate the performance of an object detection model, we need to perform the following five steps:
\begin{enumerate}
    \item Calculate the Intersection over Union (IoU) between the predicted and ground-truth bounding boxes to determine the detection's location correctness.
    \item Create a confusion matrix for each categorical label at a particular IoU and confidence threshold by comparing the predicted labels against the ground-truth labels at that configuration.
    \item Calculate the precision and recall metrics for each confusion matrix. These two metrics indicate the model's reliability and sensitivity in classifying objects of this category at each threshold pair. Additionally, these two metrics can be used to plot the precision-recall curve, which signifies the tradeoff between the two metrics across confidence thresholds. An object detection model is considered high performance at a confidence threshold if the precision remains high as recall increases. The F-score is then used to find the combination of precision and recall at which the model achieves its highest reliability and sensitivity performance across all confidence thresholds.
    \item Calculate the average precision (AP) to determine the model's overall performance in classifying a category at a specific IoU threshold.
    \item Finally, calculate the mean average precision (mAP) to assess the model's accuracy in detecting objects across all categories at a given IoU threshold.
\end{enumerate}
This section provides an overview of how we integrate metrics into our comparison software. Our comparison software focuses on assessing the performance of Mask R-CNN versus the YOLOv5 model in object detection tasks. Since the output of the two models is formatted to produce the same data structure, a list of PredictObject instances, we can further process their output in a similar manner. The following discussion will focus on evaluating the object detection performance of the Mask R-CNN model, unless otherwise specified. 

\subsection{IoU Calculation}

As stated in Sections \ref{sec:nuimage} and \ref{sec:prep_models}, the ground-truth data is provided in the form of a list of TruthObject instances, whereas each model produces a list of PredictObject instances. It should be mentioned that each item in the TruthObject list represents an object present in the image, and the list is not ordered in any particular way. On the other hand, each instance in the PredictObject list represents an object detected by the model in the image and is sorted from the most confident detection to the least confident detection. It is important to note that there is no direct correspondence between the items in the two lists with respect to their location. Therefore, it is not possible to determine which PredictObject instance corresponds to the detection of a particular TruthObject instance.

In order to map the elements in the PredictObject and TruthObject lists, we employ a greedy brute force algorithm implemented in the \inlcode{preds\_choose\_truths\_map} function in the \inlcode{src/metrics.py} module. This function takes in the two lists and produces a list of hashmaps, where each hashmap maps a PredictObject instance to a TruthObject instance and provides their corresponding Intersection over Union (IoU) value.

The main idea behind the greedy mapping algorithm is to assign the PredictObject instance to the TruthObject instance with the highest IoU score among all available TruthObject instances. The greedy algorithm follows the following two steps. Firstly, for each instance in the PredictObject list, we calculate the IoU value between that instance and each available TruthObject instance. The IoU calculation between any PredictObject instance and TruthObject instance is straightforward as they both have their bounding box's top-left and bottom-right corner coordinates. These coordinates are used to calculate the intersection and union area between the two boxes, and then the IoU as shown in the \inlcode{get\_box\_iou} function in the \inlcode{src/metrics.py} module. Secondly, the PredictObject instance is mapped to the TruthObject instance that has the highest IoU score among all the available TruthObject instances. Then, this mapped TruthObject instance is marked as used or unavailable for subsequent instances in the PredictObject list.

This greedy mapping scheme effectively achieves two goals. The first goal is to assign each PredictObject to a  TruthObject instance, one-to-one correspondence between the two lists. The second goal is to reduce the model's overall performance further when a high-confidence detection has a lower IoU score with its corresponding ground-truth object compared to when a low-confidence detection has the same IoU score with its corresponding ground-truth object. By allowing the high-confidence PredictObject instances to choose their mapping first, the lower-confidence PredictObject instances will have fewer available TruthObject instances to choose from, forcing them to have a lower priority compared to the higher-confidence PredictObject instances.