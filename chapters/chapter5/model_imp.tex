\section{Prepare Models for Comparision}  \label{sec:prep_models}

\subsection{Mask R-CNN Preparation} \label{subsec:mrcnn_prep}

For our comparison, we utilized the pre-trained Mask R-CNN model for the COCO challenge, which is implemented in the \inlcode{torchvision} library \cite{pytorch_mrcnn}. This implementation uses the ResNet50 CNN backbone, as discussed in Section \ref{sec:mask_rcnn}, and can be accessed through the \inlcode{maskrcnn\_resnet50\_fpn} function provided by the \inlcode{torchvision.models.detection} module. The function expects a list of tensors as input, where each tensor represents an image with dimensions of [number of image channels, image width, image height]. To convert an image to a tensor, we used the \inlcode{read\_image} helper function provided by the \inlcode{torchvision.io} module, which takes the path to the image storage location as an input string. Since the model uses pre-trained weights for the COCO challenge, we must apply the preprocessing operations on the image tensor provided by \inlcode{MaskRCNN\_ResNet50\_FPN\_Weights.COCO\_V1.transforms} function before passing it to the \inlcode{maskrcnn\_resnet50\_fpn} function.

The \inlcode{maskrcnn\_resnet50\_fpn} function employs the Mask R-CNN model to predict four lists -- labels, confidence scores, bounding boxes, and masks -- for each image tensor. The elements in these four lists correspond to each other location-wise and provide information about the detected objects. The list of confidence scores is sorted from highest to lowest, and the order of the other three lists changes accordingly. For instance, the first element in the scores, labels, bounding boxes, and masks list describes the detected object that the model is most confident about among all detected objects. 

To eliminate the need to maintain location correspondence between four lists, the PredictObject class, defined in \inlcode{src/predict\_obj.py}, is used. Each class instance represents a predicted object within the image and contains the object's label, confidence score, bounding box, and mask. Similar to the TruthObject, the PredictObject follows the OOP principle, enabling easy access to information without affecting program efficiency. Since the instances of this class are passed across functions as references rather than actual copies, their usage does not impact program efficiency, as documented in \cite{python3_docs}. Furthermore, since the model is pre-trained for the COCO challenge, which consists of 80 categories, we remove all objects with labels that are not in the supported labels set before creating the list of PredictObject instances. This ensures that the label comparison between the ground-truth and predicted labels is valid, and there is no need for label mapping since all the supported labels exist in the COCO challenge's label set.

\subsection{YOLOv5 Preparation}

We utilized the pre-trained YOLOv5 model developed by Ultralytics teams as the second model for our comparison. YOLOv5 has five versions, ranging from most minor to most extensive in the number of neuron nodes and depth layers. For our comparison, we used the most extensive version, known as YOLOv5x. The YOLOv5 model is available through PyTorch and can be loaded into our program using the \inlcode{hub.load} function provided in the \inlcode{torch} library. Similar to the Mask R-CNN model, we loaded the pre-trained COCO weight for the YOLOv5 model. However, unlike Mask R-CNN, once initiated in our program, the YOLOv5 model takes the path of an image and produces a Pandas data frame. Each row in the data frame represents a detected object in the image. We convert this data frame into a list of PredictObject instances, similar to the process described in Subsection \ref{subsec:mrcnn_prep}. This conversion ensures that other functions in our program can process the YOLOv5 model output in the same manner as the Mask R-CNN model output. One key difference is that YOLOv5 does not generate a mask for each object as is an object detection model; therefore, the mask field for each PredictObject instance defaults to a None value.